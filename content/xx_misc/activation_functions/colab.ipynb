{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/content/xx_misc/activation_functions/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "copyright"
   },
   "source": [
    "#### Copyright 2020 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PLP9Q30PKtv"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5W9rkuBmBu9"
   },
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIykBQbYXrXA"
   },
   "source": [
    "Activation functions are core components of neural networks. These functions are used in every node of the network to reduce a vector of inputs into an output value.\n",
    "\n",
    "Learning when to apply specific activation functions is a critical skill for building deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8u2lYRWbE37"
   },
   "source": [
    "## What Is an Activation Function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WhT2IYeaX7H8"
   },
   "source": [
    "Picture yourself as a node in a neural network. On one side of you there are multiple input streams passing data from the prior layer. On the other side there are multiple output streams that we use to pass data to every node in the next layer.\n",
    "\n",
    "We expect the data from our input layer to contain many different values since we get data from different nodes. On the output side we'll give every node in the next layer the same value. Distilling the multiple diverse inputs into a single value that we can hand to the next layer is the job of an activation function.\n",
    "\n",
    "In mathematical terms it looks something like this:\n",
    "\n",
    "$$a = activation(bias + \\sum_{i=0}^{n}{x_i})$$\n",
    "\n",
    "We sum our inputs from prior nodes, $x$, and our bias. We then pass that summation through an activation function in order to get our output value, $y$, that we then pass to every node in the next layer of the network.\n",
    "\n",
    "Though activation functions are used in every layer of a network, it is particularly important to understand how they behave at the output layer of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxY-NIdqaw-a"
   },
   "source": [
    "## Pass-Through Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9QLdlAEa8bh"
   },
   "source": [
    "The most basic activation function is the [linear](https://www.tensorflow.org/api_docs/python/tf/keras/activations/linear) activation function. This function takes the sum of inputs and bias, does nothing to it, and hands the result to the next layer of the network.\n",
    "\n",
    "Let's plot the linear activation function in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LkNEfo1RbURY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def linear(x):\n",
    "  return x\n",
    "\n",
    "inputs = np.linspace(-10, 10, 10)\n",
    "outputs = [linear(x) for x in inputs]\n",
    "_ = plt.plot(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VfQC6wHWb72g"
   },
   "source": [
    "That's a pretty simple activation function to understand. But what value does it provide?\n",
    "\n",
    "This function can be useful, especially in your output layer, if you want your model to produce large or negative values. Many of the activation functions we'll see greatly restrict the range of values that they output. The linear activation function does restrict its output range at all. Any real number can be produced by a node with this activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ov3n-R3HdgGZ"
   },
   "source": [
    "## Rectified Linear Unit (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLysbFqRdlWZ"
   },
   "source": [
    "There is another linear activation function that turns out to be quite useful: the [Rectified Linear Unit (ReLU)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu).\n",
    "\n",
    "ReLU simply returns the input value unless that value is less than zero. In that case it returns zero.\n",
    "\n",
    "$$a = \\begin{cases}\n",
    "x \\ , &x \\geq 0 \\\\\n",
    "0 \\ , &x < 0 \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Let's take a look at ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K8-miZ45dp1b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "  if x < 0:\n",
    "    return 0\n",
    "  return x\n",
    "\n",
    "inputs = np.linspace(-10, 10, 100, .1)\n",
    "outputs = [relu(x) for x in inputs]\n",
    "_ = plt.plot(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Da8UGSJoeyWR"
   },
   "source": [
    "This is also a quite simple activation, but it turns out to be quite useful in practice. Many powerful neural networks utilize ReLU activation, at least in part. It has the advantage of making training very fast; however, nodes using ReLU do run the risk of \"dying\" during the training process. The nodes die when they get to a state where they always produce a zero output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jaD3-xNhgGl1"
   },
   "source": [
    "Let's also think about the use of a ReLU node in a network. If the output layer consists of ReLU values, then the output of the network will be from `0` to infinity.\n",
    "\n",
    "This works fine for models that are predicting positive values, but what if your model is predicting celsius temperatures in Antarctica or some other potentially negative value?\n",
    "\n",
    "In this case you would need to adjust the target training data to all be positive, say by adding `100` to it, and then do the reverse to the output of the model, subtract `100` from each value.\n",
    "\n",
    "You'll find that you'll need to do this type of adjustment quite often when building models. Understanding your activation functions, especially in your output layer, is critically important. When you know the range of values that your model can produce you can adjust your training data to fall within that range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXuakqxriZJQ"
   },
   "source": [
    "## Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3HW8fiPSibHV"
   },
   "source": [
    "We talked about dead nodes when discussing the ReLU activation function. One strategy that helps mitigate the dead node issue is a \"leaky\" ReLU.  Leaky ReLUs are ReLU functions that pass through any value zero or greater. For values less than zero they apply an alpha value to them and return the result.\n",
    "\n",
    "$$a = \\begin{cases}\n",
    "x \\ , &x \\geq 0 \\\\\n",
    "x * \\alpha \\ , &x < 0 \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "TensorFlow Keras doen't make a distinction between ReLU and Leaky ReLU, it simply provides an alpha parameter to [relu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pa3RVEFkjq_X"
   },
   "source": [
    "### Exercise 1: Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "recLkGjzjtD2"
   },
   "source": [
    "Write a `leaky_relu` function that passes through any value zero or greater and applies an alpha of `0.1` to values less than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HaPaIGF2j6Zf"
   },
   "source": [
    "**Student Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9kclhS0QjeOb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def leaky_relu(x):\n",
    "  pass # Your code goes here\n",
    "\n",
    "inputs = np.linspace(-10, 10, 100, .1)\n",
    "outputs = [leaky_relu(x) for x in inputs]\n",
    "_ = plt.plot(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJ6CXhn_kIBn"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2UCbhUpukY1K"
   },
   "source": [
    "## Binary Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TZpDhyYmkbq-"
   },
   "source": [
    "The binary step activation function serves as an on/off switch for a node. This function returns zero if its input is on one side of a threshold and returns one if it is on the other.\n",
    "\n",
    "$$a = \\begin{cases}\n",
    "1 \\ , &x \\geq 0 \\\\\n",
    "0 \\ , &x < 0 \\\\\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfHXM-Sjmk_K"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def binary_step(x):\n",
    "  if x < 0:\n",
    "    return 0\n",
    "  return 1\n",
    "\n",
    "inputs = np.linspace(-10, 10, 100, .1)\n",
    "outputs = [binary_step(x) for x in inputs]\n",
    "_ = plt.plot(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Rk7qA21oVG0"
   },
   "source": [
    "At the output layer this function can be useful when you need to make a yes/no decision and don't care about the confidence of the model in that decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xOw5sUIfpmmI"
   },
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "39hbHBRKppPE"
   },
   "source": [
    "Activation functions can also be non-linear. The [sigmoid](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid) function works using a logistic curve.\n",
    "\n",
    "$$a=\\frac{1}{1+e^{-x}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZ7bDksKp8Zl"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "inputs = np.linspace(-10, 10, 100, .1)\n",
    "outputs = [sigmoid(x) for x in inputs]\n",
    "_ = plt.plot(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJohf_AlqRLp"
   },
   "source": [
    "You'll notice that the sigmoid function restricts its output range to $(0.0, 1.0)$. This is typically not a concern in hidden layers, but needs to be considered in the output layer. You'll likely need to scale your training targets down to this range and expand your predictions back to your actual data range.\n",
    "\n",
    "Sigmoids in the output layer can be very useful for predicting continuous values. They can also be useful when making binary classification decisions. You can build a model that outputs values from $(0.0, 1.0)$ and treat the output as a confidence in a decision where values closer to `0.0` show no confidence and  values closer to `1.0` show extreme confidence. You then experiment and set a threshold where you make your binary decision.\n",
    "\n",
    "For example, if you were making a classifier to determine if an image contained a cat you might find that any time the model returned a value over `0.85` there was typically a cat in the image. Before making this decision, you'd need to experiment, find the precision and recall for different thresholds, and choose the one that fits your use case the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2gR1zW4Tt_59"
   },
   "source": [
    "## Hyperbolic Tangent (tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzZpEmsippKH"
   },
   "source": [
    "Similar to sigmoid, the hyperbolic tangent, [tanh](https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh) is a non-linear activation function that can be used in your models. The biggest difference between sigmoid and tanh is that tanh has an output range of $(-1.0, 1.0)$\n",
    "\n",
    "$$a=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMtF5swxu3pM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def tanh(x):\n",
    "  return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "inputs = np.linspace(-10, 10, 100, .1)\n",
    "outputs = [tanh(x) for x in inputs]\n",
    "_ = plt.plot(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Vi3O347bIWz"
   },
   "source": [
    "The tanh function is generally useful in hidden layers and can be especially useful in output layers where you need to produce negative numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLKmhdd5vvk-"
   },
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-ZpEmI2wBvI"
   },
   "source": [
    "So far all of the activation functions we have seen operate without knowing anything about other nodes in their layer. Each node accepts input from the layer before it and passes output to the next layer in the model. The node is unaware of any other node in its own layer, and activation functions on the nodes work independently.\n",
    "\n",
    "[Softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) is a different type of activation function. Softmax is aware of nodes in the same layer and adjusts their outputs in relation to each other.\n",
    "\n",
    "Softmax outputs values in the range of $[0.0, 1.0]$. If you were to sum the outputs of every node in a layer, the sum would always equal `1.0`, or something very very close to `1.0`.\n",
    "\n",
    "Let's say we had a model that tried to determine if an image contained an apple, orange, or grapefruit. If given a picture of a bright red apple, it might output `[1.0, 0.0, 0.0]` to show that it was highly confident that the image contained an apple. If given a picture of a yellow apple, it might be a little less confident and output `[0.8, 0.15, 0.05]`, indicating a little less confidence. If given a picture of a large orange it might output `[0.05, 0.55, 0.4]`, showing that it was having a tough time making a decision.\n",
    "\n",
    "It is worth noting that softmax is typically not used in hidden layers of a model. Most of the time you will see it used on the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYZEXNK1VDIJ"
   },
   "source": [
    "## Exercise 2: Which Activation Function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wE4hZMXqz8hv"
   },
   "source": [
    "In this exercise we will describe a model that we are building, and you will answer with the best activation function to use in the output layer and why. Be sure to talk about what your output data represents and how it will be interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmACNxgV0xhN"
   },
   "source": [
    "1. We are building a model that predicts the stock price for a stock. Which activation function should we use in the output layer and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXsKVb_u0ZkH"
   },
   "source": [
    "> *Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YiH_-6uz00WX"
   },
   "source": [
    "2. We are building a model that classifies a lung scan image as having pneumonia or not. Which activation function should we use in the output layer and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "usK5HzJe1PXb"
   },
   "source": [
    "> *Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxhefN5D1U7p"
   },
   "source": [
    "3. We are building a model that determines if an image of a number is 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. Which activation function should we use in the output layer and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqmOXCTe1jZB"
   },
   "source": [
    "> *Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wX5GCEVy1l_Q"
   },
   "source": [
    "4. We are building a model that predicts the daily change in temperature at a location. Which activation function should we use in the output layer and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qHN-Y5yN2Oqm"
   },
   "source": [
    "> *Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nsKsUW7t3IIK"
   },
   "source": [
    "5. We are building a model that attempts to predict which single Unicode character is depicted in an image. Which activation function should we use in the output layer and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMcVaflf3VC_"
   },
   "source": [
    "> *Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uby3SnIl2R9U"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "copyright",
    "Swt2fxm-fG_B",
    "iWq38ASlb2aY",
    "CYZEXNK1VDIJ",
    "exercise-1-key-1"
   ],
   "include_colab_link": true,
   "name": "Activation Functions",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
